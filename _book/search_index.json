[
["index.html", "Coding Causal Inference in R About", " Coding Causal Inference in R 2019-03-29 About This notebook contains R code for part 2 of Causal Inference by Miguel Hernán and Jamie Robins (1). While R, SAS, Stata, and Python code are available on the website for Causal Inference, we focus on doing causal inference using the tidyverse ecosystem of R packages, particularly ggplot2, dplyr, and broom. This notebook is hosted on GitHub; please post any issues or suggestions there. References "],
["chapter-11-why-model.html", "1 Chapter 11: Why model? 1.1 Program 11.1 1.2 Program 11.2 1.3 Program 11.3", " 1 Chapter 11: Why model? This is the code for Chapter 11. Throughout this chapter and the rest of the book, we’ll use the tidyverse metapackage to load the core tidyverse packages, as well the broom package. In this chapter, the main packages we’ll be using are ggplot2 for data visualization, dplyr for data manipulation, and broom for tidying regression model results. 1.1 Program 11.1 To replicate Program 11.1, we first need to create the data set, binary_a_df. library(tidyverse) library(broom) binary_a_df &lt;- data.frame( a = c(rep(1, 8), rep(0, 8)), y = c(200, 150, 220, 110, 50, 180, 90, 170, 170, 30, 70, 110, 80, 50, 10, 20) ) ggplot(binary_a_df, aes(a, y)) + geom_point(size = 4, col = &quot;white&quot;, fill = &quot;#E69F00&quot;, shape = 21) + scale_x_continuous(breaks = c(0, 1), expand = expand_scale(.5)) + theme_minimal(base_size = 20) FIGURE 1.1: Figure 11.1 To summarize the data, we’ll use dplyr to group the dataset by a, then get the sample size, mean, standard deviation, and range. knitr::kable() is used to print the results nicely to a table. (For more information on the %&gt;% pipe operator, see R for Data Science.) binary_a_df %&gt;% group_by(a) %&gt;% summarize( n = n(), mean = mean(y), sd = sd(y), minimum = min(y), maximum = max(y) ) %&gt;% knitr::kable(digits = 2) a n mean sd minimum maximum 0 8 67.50 53.12 10 170 1 8 146.25 58.29 50 220 Similarly, we’ll plot and summarize categorical_a_df. categorical_a_df &lt;- data.frame(a = sort(rep(1:4, 4)), y = c(110, 80, 50, 40, 170, 30, 70, 50, 110, 50, 180, 130, 200, 150, 220, 210)) ggplot(categorical_a_df, aes(a, y)) + geom_point(size = 4, col = &quot;white&quot;, fill = &quot;#E69F00&quot;, shape = 21) + scale_x_continuous(breaks = 1:4, expand = expand_scale(.25)) + theme_minimal(base_size = 20) FIGURE 1.2: Figure 11.2 categorical_a_df %&gt;% group_by(a) %&gt;% summarize( n = n(), mean = mean(y), sd = sd(y), minimum = min(y), maximum = max(y) ) %&gt;% knitr::kable(digits = 2) a n mean sd minimum maximum 1 4 70.0 31.62 40 110 2 4 80.0 62.18 30 170 3 4 117.5 53.77 50 180 4 4 195.0 31.09 150 220 1.2 Program 11.2 Program 11.2 uses a continuous exposure and outcome data, continuous_a_df. Plotting the points is similar to above. continuous_a_df &lt;- data.frame( a = c(3, 11, 17, 23, 29, 37, 41, 53, 67, 79, 83, 97, 60, 71, 15, 45), y = c(21, 54, 33, 101, 85, 65, 157, 120, 111, 200, 140, 220, 230, 217, 11, 190) ) ggplot(continuous_a_df, aes(a, y)) + geom_point(size = 4, col = &quot;white&quot;, fill = &quot;#E69F00&quot;, shape = 21) + theme_minimal(base_size = 20) FIGURE 1.3: Figure 11.3 We’ll also add a regression line using geom_smooth() with method = &quot;lm&quot;. ggplot(continuous_a_df, aes(a, y)) + geom_point(size = 4, col = &quot;white&quot;, fill = &quot;grey85&quot;, shape = 21) + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;#E69F00&quot;, size = 1.2) + theme_minimal(base_size = 20) FIGURE 1.4: Figure 11.4 To fit an OLS regression model of a on y, we’ll uselm() and then tidy the results with tidy() from the broom package. linear_regression &lt;- lm(y ~ a, data = continuous_a_df) linear_regression %&gt;% # get the confidence intervals using `conf.int = TRUE` tidy(conf.int = TRUE) %&gt;% # drop the test statistic and P-value select(-statistic, -p.value) %&gt;% knitr::kable(digits = 2) term estimate std.error conf.low conf.high (Intercept) 24.55 21.33 -21.20 70.29 a 2.14 0.40 1.28 2.99 To predict a value of y for when a = 90, we pass the linear regression model object linear_regression to the predict() function. We also give it a new data frame that contains only one value, a = 90. linear_regression %&gt;% predict(newdata = data.frame(a = 90)) ## 1 ## 216.89 Similarly, using binary_a_df, which has a treatmant a that is a binary: lm(y ~ a, data = binary_a_df) %&gt;% tidy(conf.int = TRUE) %&gt;% select(-statistic, -p.value) %&gt;% knitr::kable(digits = 2) term estimate std.error conf.low conf.high (Intercept) 67.50 19.72 25.21 109.79 a 78.75 27.88 18.95 138.55 1.3 Program 11.3 Fitting and tidying a quadratic version of a is similar, but we use I() to include a^2. smoothed_regression &lt;- lm(y ~ a + I(a^2), data = continuous_a_df) smoothed_regression %&gt;% tidy(conf.int = TRUE) %&gt;% select(-statistic, -p.value) %&gt;% # remove `I()` from the term name mutate(term = ifelse(term == &quot;I(a^2)&quot;, &quot;a^2&quot;, term)) %&gt;% knitr::kable(digits = 2) term estimate std.error conf.low conf.high (Intercept) -7.41 31.75 -75.99 61.18 a 4.11 1.53 0.80 7.41 a^2 -0.02 0.02 -0.05 0.01 To plot the quadratic function, we give geom_smooth() a formula argument: formula = y ~ x + I(x^2). ggplot(continuous_a_df, aes(a, y)) + geom_point(size = 4, col = &quot;white&quot;, fill = &quot;grey85&quot;, shape = 21) + geom_smooth(method = &quot;lm&quot;, se = FALSE, col = &quot;#E69F00&quot;, formula = y ~ x + I(x^2), size = 1.2) + theme_minimal(base_size = 20) FIGURE 1.5: Figure 11.5 But predicting is done the same way. smoothed_regression %&gt;% predict(newdata = data.frame(a = 90)) ## 1 ## 197.1269 "],
["chapter-12-ip-weighting-and-marginal-structural-models.html", "2 Chapter 12: IP Weighting and Marginal Structural Models 2.1 Program 12.1 2.2 Program 12.2 2.3 Program 12.3 2.4 Program 12.4 2.5 Program 12.5 2.6 Program 12.6 2.7 Program 12.7", " 2 Chapter 12: IP Weighting and Marginal Structural Models This is the code for Chapter 12. As before, we’ll use the tidyverse metapackage and broom, as well as haven, for reading files from SAS (and other statistical software) and tableone for creating descriptive tables. We’ll also use the estimatr package for a robust version of lm(), geepack for robust generalized estimating equations modeling, and the boot package to help with bootstrapping confidence intervals. The data is available to download on the Causal Inference website. 2.1 Program 12.1 library(tidyverse) library(haven) library(broom) library(tableone) library(estimatr) library(geepack) library(boot) Causal Inference uses data from NHEFS. To read in the SAS file, use read_sas() from the haven package. # read the SAS data file nhefs &lt;- read_sas(&quot;data/nhefs.sas7bdat&quot;) nhefs ## # A tibble: 1,629 x 64 ## seqn qsmk death yrdth modth dadth sbp dbp sex age race income ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 233 0 0 NA NA NA 175 96 0 42 1 19 ## 2 235 0 0 NA NA NA 123 80 0 36 0 18 ## 3 244 0 0 NA NA NA 115 75 1 56 1 15 ## 4 245 0 1 85 2 14 148 78 0 68 1 15 ## 5 252 0 0 NA NA NA 118 77 0 40 0 18 ## 6 257 0 0 NA NA NA 141 83 1 43 1 11 ## 7 262 0 0 NA NA NA 132 69 1 56 0 19 ## 8 266 0 0 NA NA NA 100 53 1 29 0 22 ## 9 419 0 1 84 10 13 163 79 0 51 0 18 ## 10 420 0 1 86 10 17 184 106 0 43 0 16 ## # … with 1,619 more rows, and 52 more variables: marital &lt;dbl&gt;, ## # school &lt;dbl&gt;, education &lt;dbl&gt;, ht &lt;dbl&gt;, wt71 &lt;dbl&gt;, wt82 &lt;dbl&gt;, ## # wt82_71 &lt;dbl&gt;, birthplace &lt;dbl&gt;, smokeintensity &lt;dbl&gt;, ## # smkintensity82_71 &lt;dbl&gt;, smokeyrs &lt;dbl&gt;, asthma &lt;dbl&gt;, bronch &lt;dbl&gt;, ## # tb &lt;dbl&gt;, hf &lt;dbl&gt;, hbp &lt;dbl&gt;, pepticulcer &lt;dbl&gt;, colitis &lt;dbl&gt;, ## # hepatitis &lt;dbl&gt;, chroniccough &lt;dbl&gt;, hayfever &lt;dbl&gt;, diabetes &lt;dbl&gt;, ## # polio &lt;dbl&gt;, tumor &lt;dbl&gt;, nervousbreak &lt;dbl&gt;, alcoholpy &lt;dbl&gt;, ## # alcoholfreq &lt;dbl&gt;, alcoholtype &lt;dbl&gt;, alcoholhowmuch &lt;dbl&gt;, ## # pica &lt;dbl&gt;, headache &lt;dbl&gt;, otherpain &lt;dbl&gt;, weakheart &lt;dbl&gt;, ## # allergies &lt;dbl&gt;, nerves &lt;dbl&gt;, lackpep &lt;dbl&gt;, hbpmed &lt;dbl&gt;, ## # boweltrouble &lt;dbl&gt;, wtloss &lt;dbl&gt;, infection &lt;dbl&gt;, active &lt;dbl&gt;, ## # exercise &lt;dbl&gt;, birthcontrol &lt;dbl&gt;, pregnancies &lt;dbl&gt;, ## # cholesterol &lt;dbl&gt;, hightax82 &lt;dbl&gt;, price71 &lt;dbl&gt;, price82 &lt;dbl&gt;, ## # tax71 &lt;dbl&gt;, tax82 &lt;dbl&gt;, price71_82 &lt;dbl&gt;, tax71_82 &lt;dbl&gt; First, we need to clean up the data a little. There’s already a variable that could be an ID, seqn, but we’ll make a simpler one, id. We’re also going to add a variable called censored that is 1 if the weight variable from 1982 is missing and 0 otherwise. We’ll also create two categorical variables from age and school: older, a binary variable indicating if the person is older than 50, and education, a categorical variable representing years of education. Finally, we’ll change all of the categorical variables to have be factors. nhefs &lt;- nhefs %&gt;% mutate( # add id and censored indicator id = 1:n(), censored = ifelse(is.na(wt82), 1, 0), # recode age &gt; 50 and years of school to categories older = case_when( is.na(age) ~ NA_real_, age &gt; 50 ~ 1, TRUE ~ 0 ), education = case_when( school &lt; 9 ~ 1, school &lt; 12 ~ 2, school == 12 ~ 3, school &lt; 16 ~ 4, TRUE ~ 5 ) ) %&gt;% # change categorical variables to factors mutate_at(vars(sex, race, education, exercise, active), factor) For the analysis, we’ll only use participants with complete covariate data and drop the rest using drop_na() from the tidyr package. # restrict to complete cases nhefs_complete &lt;- nhefs %&gt;% drop_na(qsmk, sex, race, age, school, smokeintensity, smokeyrs, exercise, active, wt71, wt82, wt82_71, censored) Then we’ll summarize the mean and SD for the difference in weight between 1982 and 1971, grouped by whether or not the participant quit smoking. nhefs_complete %&gt;% # only show for pts not lost to follow-up filter(censored == 0) %&gt;% group_by(qsmk) %&gt;% summarize( mean_weight_change = mean(wt82_71), sd = sd(wt82_71) ) %&gt;% knitr::kable(digits = 2) qsmk mean_weight_change sd 0 1.98 7.45 1 4.53 8.75 To recreate Table 12.1, we’ll use the tableone package, which easily creates descriptive tables. First, we’ll clean up the data a little more to have better labels for variable names and the levels within each variable. Then, we pass tbl1_data to CreateTableOne() and print it as a kable. # a helper function to turn into Yes/No factor fct_yesno &lt;- function(x) { factor(x, labels = c(&quot;No&quot;, &quot;Yes&quot;)) } tbl1_data &lt;- nhefs_complete %&gt;% # filter out participants lost to follow-up filter(censored == 0) %&gt;% # turn categorical variables into factors mutate( university = fct_yesno(ifelse(education == 5, 1, 0)), no_exercise = fct_yesno(ifelse(exercise == 2, 1, 0)), inactive = fct_yesno(ifelse(active == 2, 1, 0)), qsmk = factor(qsmk, levels = 1:0, c(&quot;Ceased Smoking&quot;, &quot;Continued Smoking&quot;)), sex = factor(sex, levels = 1:0, labels = c(&quot;Female&quot;, &quot;Male&quot;)), race = factor(race, levels = 1:0, labels = c(&quot;Other&quot;, &quot;White&quot;)) ) %&gt;% # only include a subset of variables in the descriptive tbl select(qsmk, age, sex, race, university, wt71, smokeintensity, smokeyrs, no_exercise, inactive) %&gt;% # rename variable names to match Table 12.1 rename( &quot;Smoking Cessation&quot; = &quot;qsmk&quot;, &quot;Age&quot; = &quot;age&quot;, &quot;Sex&quot; = &quot;sex&quot;, &quot;Race&quot; = &quot;race&quot;, &quot;University education&quot; = &quot;university&quot;, &quot;Weight, kg&quot; = &quot;wt71&quot;, &quot;Cigarettes/day&quot; = &quot;smokeintensity&quot;, &quot;Years smoking&quot; = &quot;smokeyrs&quot;, &quot;Little or no exercise&quot; = &quot;no_exercise&quot;, &quot;Inactive daily life&quot; = &quot;inactive&quot; ) tbl1_data %&gt;% # create a descriptive table CreateTableOne( # pull all variable names but smoking vars = select(tbl1_data, -`Smoking Cessation`) %&gt;% names, # stratify by smoking status strata = &quot;Smoking Cessation&quot;, # use `.` to direct the pipe to the `data` argument data = ., # don&#39;t show p-values test = FALSE ) %&gt;% # print to a kable kableone() Ceased Smoking Continued Smoking n 403 1163 Age (mean (SD)) 46.17 (12.21) 42.79 (11.79) Sex = Male (%) 220 (54.6) 542 (46.6) Race = White (%) 367 (91.1) 993 (85.4) University education = Yes (%) 62 (15.4) 115 ( 9.9) Weight, kg (mean (SD)) 72.35 (15.63) 70.30 (15.18) Cigarettes/day (mean (SD)) 18.60 (12.40) 21.19 (11.48) Years smoking (mean (SD)) 26.03 (12.74) 24.09 (11.71) Little or no exercise = Yes (%) 164 (40.7) 441 (37.9) Inactive daily life = Yes (%) 45 (11.2) 104 ( 8.9) 2.2 Program 12.2 Now, we’ll fit the weights for the marginal structural model. For logistic regression, we’ll use glm() to fit a model called propensity_model. # estimation of IP weights via a logistic model propensity_model &lt;- glm( qsmk ~ sex + race + age + I(age^2) + education + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), family = binomial(), data = nhefs_complete ) To see the coefficients of the propensity score model: propensity_model %&gt;% # get confidence intervals and exponentiate estimates tidy(conf.int = TRUE, exponentiate = TRUE) %&gt;% select(-statistic, -p.value) %&gt;% knitr::kable(digits = 2) term estimate std.error conf.low conf.high (Intercept) 0.11 1.38 0.01 1.56 sex1 0.59 0.15 0.44 0.80 race1 0.43 0.21 0.28 0.64 age 1.13 0.05 1.02 1.25 I(age^2) 1.00 0.00 1.00 1.00 education2 0.97 0.20 0.66 1.43 education3 1.09 0.18 0.77 1.55 education4 1.07 0.27 0.62 1.81 education5 1.61 0.23 1.03 2.51 smokeintensity 0.93 0.02 0.90 0.95 I(smokeintensity^2) 1.00 0.00 1.00 1.00 smokeyrs 0.93 0.03 0.88 0.98 I(smokeyrs^2) 1.00 0.00 1.00 1.00 exercise1 1.43 0.18 1.01 2.04 exercise2 1.49 0.19 1.03 2.15 active1 1.03 0.13 0.80 1.34 active2 1.19 0.21 0.78 1.81 wt71 0.98 0.03 0.94 1.04 I(wt71^2) 1.00 0.00 1.00 1.00 To predict the weights, we’ll use the augment() function from broom to add the predicted probabilities of quitting smoking (called .fitted by default) to nhefs_complete. What we actually need is the probability for each person’s observed outcome, so for people who did not quit smoking, we need 1 - .fitted. Using mutate(), we’ll add a variable called wts, which is 1 divided by this probability. nhefs_complete &lt;- propensity_model %&gt;% augment(type.predict = &quot;response&quot;, data = nhefs_complete) %&gt;% mutate(wts = 1 / ifelse(qsmk == 0, 1 - .fitted, .fitted)) It’s important to look at the distribution of the weights to see its shape and if there are any extreme values. nhefs_complete %&gt;% summarize(mean_wt = mean(wts), sd_wts = sd(wts)) ## # A tibble: 1 x 2 ## mean_wt sd_wts ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.00 1.47 ggplot(nhefs_complete, aes(wts)) + geom_density(col = &quot;#E69F00&quot;, fill = &quot;#E69F0095&quot;) + # use a log scale for the x axis scale_x_log10() + theme_minimal(base_size = 20) + xlab(&quot;log10(Weights)&quot;) While OLS regression, using lm() with weights = wts, will work fine for the estimate, the standard errors tend to be too small when we use weights. We’ll get the confidence intervals using for approaches: OLS, GEE, OLS with robust standard errors, and bootstrapped confidence intervals. tidy_est_cis() is a helper function to get the estimate and confidence intervals for each model. tidy_est_cis &lt;- function(.df, .type) { .df %&gt;% # add the name of the model to the data mutate(type = .type) %&gt;% filter(term == &quot;qsmk&quot;) %&gt;% select(type, estimate, conf.low, conf.high) } # standard error a little too small ols_cis &lt;- lm( wt82_71 ~ qsmk, data = nhefs_complete, # weight by inverse probability weights = wts ) %&gt;% tidy(conf.int = TRUE) %&gt;% tidy_est_cis(&quot;ols&quot;) ols_cis ## # A tibble: 1 x 4 ## type estimate conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ols 3.44 2.64 4.24 geeglm() from geepack fits a GEE GLM using robust standard errors. We also need to specify the correlation structure and id variable. gee_model &lt;- geeglm( wt82_71 ~ qsmk, data = nhefs_complete, std.err = &quot;san.se&quot;, # default robust SE weights = wts, # inverse probability weights id = id, # required ID variable corstr = &quot;independence&quot; # default independent correlation structure ) gee_model_cis &lt;- tidy(gee_model, conf.int = TRUE) %&gt;% tidy_est_cis(&quot;gee&quot;) gee_model_cis ## # A tibble: 1 x 4 ## type estimate conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gee 3.44 2.41 4.47 lm_robust() from the estimatr package fits an OLS model but produces robust standard errors by default. # easy robust SEs robust_lm_model_cis &lt;- lm_robust( wt82_71 ~ qsmk, data = nhefs_complete, weights = wts ) %&gt;% tidy() %&gt;% tidy_est_cis(&quot;robust ols&quot;) robust_lm_model_cis ## type estimate conf.low conf.high ## 1 robust ols 3.440535 2.407886 4.473185 While traditional OLS gives confidence intervals that are a little to narrow, the robust methods give confidence intervals that are a little too wide. Bootstrapping gives CIs somewhere between, but to produce the right CIs, you need to bootstrap the entire fitting process, including the weights. We’ll use the boot package and write a function called model_nhefs() to fit the weights and marginal structural model. The output for model_nhefs() is the coefficient for qsmk in the marginal structural model. model_nhefs &lt;- function(data, indices) { # use bootstrapped data df &lt;- data[indices, ] # need to bootstrap the entire fitting process, including IPWs propensity &lt;- glm(qsmk ~ sex + race + age + I(age^2) + education + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), family = binomial(), data = df) df &lt;- propensity %&gt;% augment(type.predict = &quot;response&quot;, data = df) %&gt;% mutate(wts = 1 / ifelse(qsmk == 0, 1 - .fitted, .fitted)) lm(wt82_71 ~ qsmk, data = df, weights = wts) %&gt;% tidy() %&gt;% filter(term == &quot;qsmk&quot;) %&gt;% # output the coefficient for `qsmk` pull(estimate) } To get bias-corrected CIs, we’ll use 2000 bootstrap replications. # set seed for the bootstrapped confidence intervals set.seed(1234) bootstrap_estimates &lt;- nhefs_complete %&gt;% # remove the variables added by `augment()` earlier select(-.fitted:-wts) %&gt;% boot(model_nhefs, R = 2000) bootstrap_cis &lt;- bootstrap_estimates %&gt;% tidy(conf.int = TRUE, conf.method = &quot;bca&quot;) %&gt;% mutate(type = &quot;bootstrap&quot;) %&gt;% # rename `statistic` to match the other models select(type, estimate = statistic, conf.low, conf.high) bootstrap_cis ## # A tibble: 1 x 4 ## type estimate conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 bootstrap 3.44 2.47 4.43 The estimates are all the same, but the CIs vary a bit by method: the GEE and robust OLS CIs are a bit wider, and the traditional OLS CIs are smaller, with the bootstrapped CIs between. bind_rows( ols_cis, gee_model_cis, robust_lm_model_cis, bootstrap_cis ) %&gt;% # calculate CI width to sort by it mutate(width = conf.high - conf.low) %&gt;% arrange(width) %&gt;% # fix the order of the model types for the plot mutate(type = fct_inorder(type)) %&gt;% ggplot(aes(x = type, y = estimate, ymin = conf.low, ymax = conf.high)) + geom_pointrange(color = &quot;#0172B1&quot;, size = 1, fatten = 3) + coord_flip() + theme_minimal(base_size = 20) 2.3 Program 12.3 Fitting stabilized weights is similar to inverse weights, but we need to fit a model for the numerator. To fit a model with no covariates, we can just put 1 on the right hand side, e.g. qsmk ~ 1. Predicting the probabilities for this model is the same as above. We’ll use augment() and left_join() to add the numerator probabilities to nhefs_complete, then divide numerator by the probabilities fit in Program 12.2 to get stabilized weights. numerator &lt;- glm(qsmk ~ 1, data = nhefs_complete, family = binomial()) nhefs_complete &lt;- numerator %&gt;% augment(type.predict = &quot;response&quot;, data = nhefs_complete) %&gt;% mutate(numerator = ifelse(qsmk == 0, 1 - .fitted, .fitted)) %&gt;% # take just the numerator probabilities select(id, numerator) %&gt;% # join numerator probabilities to `nhefs_complete` left_join(nhefs_complete, by = &quot;id&quot;) %&gt;% # create stabilized weights mutate(swts = numerator / ifelse(qsmk == 0, 1 - .fitted, .fitted)) For stabilized weights, we want the mean to be about 1. nhefs_complete %&gt;% summarize(mean_wt = mean(swts), sd_wts = sd(swts)) ## # A tibble: 1 x 2 ## mean_wt sd_wts ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.999 0.288 ggplot(nhefs_complete, aes(swts)) + geom_density(col = &quot;#E69F00&quot;, fill = &quot;#E69F0095&quot;) + scale_x_log10() + theme_minimal(base_size = 20) + xlab(&quot;log10(Stabilized Weights)&quot;) Even though it’s a little conservative, we’ll fit the marginal structural model with robust OLS. lm_robust(wt82_71 ~ qsmk, data = nhefs_complete, weights = swts) %&gt;% tidy() ## term estimate std.error statistic p.value conf.low conf.high ## 1 (Intercept) 1.779978 0.2248362 7.916778 4.581734e-15 1.338966 2.220990 ## 2 qsmk 3.440535 0.5264638 6.535179 8.573524e-11 2.407886 4.473185 ## df outcome ## 1 1564 wt82_71 ## 2 1564 wt82_71 2.4 Program 12.4 The workflow for continuous exposures is very similar to binary exposure. The main differences are that the model needs to be appropriate for continuous variable and in how the weights are calculated. We fit the models for smoking intensity, a continuous exposure, using OLS with lm(): one for the numerator without predictors and one for the denominator with the confounders. Then, we use augment to get the predicted values for smoking intensity (.fitted) and their standard error (.sigma). Using the template dnorm(true_value, predicted_value, mean(standard_error, rm.na = TRUE)), we can fit values to use for the stabilized weights. nhefs_light_smokers &lt;- nhefs %&gt;% drop_na(qsmk, sex, race, age, school, smokeintensity, smokeyrs, exercise, active, wt71, wt82, wt82_71, censored) %&gt;% filter(smokeintensity &lt;= 25) nhefs_light_smokers ## # A tibble: 1,162 x 67 ## seqn qsmk death yrdth modth dadth sbp dbp sex age race income ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 235 0 0 NA NA NA 123 80 0 36 0 18 ## 2 244 0 0 NA NA NA 115 75 1 56 1 15 ## 3 245 0 1 85 2 14 148 78 0 68 1 15 ## 4 252 0 0 NA NA NA 118 77 0 40 0 18 ## 5 257 0 0 NA NA NA 141 83 1 43 1 11 ## 6 262 0 0 NA NA NA 132 69 1 56 0 19 ## 7 266 0 0 NA NA NA 100 53 1 29 0 22 ## 8 419 0 1 84 10 13 163 79 0 51 0 18 ## 9 420 0 1 86 10 17 184 106 0 43 0 16 ## 10 434 0 0 NA NA NA 127 80 1 54 0 16 ## # … with 1,152 more rows, and 55 more variables: marital &lt;dbl&gt;, ## # school &lt;dbl&gt;, education &lt;fct&gt;, ht &lt;dbl&gt;, wt71 &lt;dbl&gt;, wt82 &lt;dbl&gt;, ## # wt82_71 &lt;dbl&gt;, birthplace &lt;dbl&gt;, smokeintensity &lt;dbl&gt;, ## # smkintensity82_71 &lt;dbl&gt;, smokeyrs &lt;dbl&gt;, asthma &lt;dbl&gt;, bronch &lt;dbl&gt;, ## # tb &lt;dbl&gt;, hf &lt;dbl&gt;, hbp &lt;dbl&gt;, pepticulcer &lt;dbl&gt;, colitis &lt;dbl&gt;, ## # hepatitis &lt;dbl&gt;, chroniccough &lt;dbl&gt;, hayfever &lt;dbl&gt;, diabetes &lt;dbl&gt;, ## # polio &lt;dbl&gt;, tumor &lt;dbl&gt;, nervousbreak &lt;dbl&gt;, alcoholpy &lt;dbl&gt;, ## # alcoholfreq &lt;dbl&gt;, alcoholtype &lt;dbl&gt;, alcoholhowmuch &lt;dbl&gt;, ## # pica &lt;dbl&gt;, headache &lt;dbl&gt;, otherpain &lt;dbl&gt;, weakheart &lt;dbl&gt;, ## # allergies &lt;dbl&gt;, nerves &lt;dbl&gt;, lackpep &lt;dbl&gt;, hbpmed &lt;dbl&gt;, ## # boweltrouble &lt;dbl&gt;, wtloss &lt;dbl&gt;, infection &lt;dbl&gt;, active &lt;fct&gt;, ## # exercise &lt;fct&gt;, birthcontrol &lt;dbl&gt;, pregnancies &lt;dbl&gt;, ## # cholesterol &lt;dbl&gt;, hightax82 &lt;dbl&gt;, price71 &lt;dbl&gt;, price82 &lt;dbl&gt;, ## # tax71 &lt;dbl&gt;, tax82 &lt;dbl&gt;, price71_82 &lt;dbl&gt;, tax71_82 &lt;dbl&gt;, id &lt;int&gt;, ## # censored &lt;dbl&gt;, older &lt;dbl&gt; denominator_model &lt;- lm(smkintensity82_71 ~ sex + race + age + I(age^2) + education + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), data = nhefs_light_smokers) denominators &lt;- denominator_model %&gt;% augment(data = nhefs_light_smokers) %&gt;% mutate(denominator = dnorm(smkintensity82_71, .fitted, mean(.sigma, na.rm = TRUE))) %&gt;% select(id, denominator) numerator_model &lt;- lm(smkintensity82_71 ~ 1, data = nhefs_light_smokers) numerators &lt;- numerator_model %&gt;% augment(data = nhefs_light_smokers) %&gt;% mutate(numerator = dnorm(smkintensity82_71, .fitted, mean(.sigma, na.rm = TRUE))) %&gt;% select(id, numerator) nhefs_light_smokers &lt;- nhefs_light_smokers %&gt;% left_join(numerators, by = &quot;id&quot;) %&gt;% left_join(denominators, by = &quot;id&quot;) %&gt;% mutate(swts = numerator / denominator) As with binary exposures, we want to check the distribution of our weights for a mean of around 1 and look for any extreme weights. ggplot(nhefs_light_smokers, aes(swts)) + geom_density(col = &quot;#E69F00&quot;, fill = &quot;#E69F0095&quot;) + scale_x_log10() + theme_minimal(base_size = 20) + xlab(&quot;log10(Stabilized Weights)&quot;) Fitting the marginal structural model follow the same pattern as above. smk_intensity_model &lt;- lm_robust(wt82_71 ~ smkintensity82_71 + I(smkintensity82_71^2), data = nhefs_light_smokers, weights = swts) smk_intensity_model %&gt;% tidy() ## term estimate std.error statistic p.value ## 1 (Intercept) 2.004524173 0.304377051 6.585661 6.851793e-11 ## 2 smkintensity82_71 -0.108988851 0.032772315 -3.325638 9.097994e-04 ## 3 I(smkintensity82_71^2) 0.002694946 0.002625509 1.026447 3.048951e-01 ## conf.low conf.high df outcome ## 1 1.407332469 2.601715878 1159 wt82_71 ## 2 -0.173288557 -0.044689144 1159 wt82_71 ## 3 -0.002456336 0.007846227 1159 wt82_71 To calculate the contrasts for smoking intensity values of 0 and 20, we’ll write the function calculate_contrast(). We can then bootstrap the confidence intervals. (Here, we don’t need to fit the entire process again: just the marginal structural model). calculate_contrast &lt;- function(.coefs, x) { .coefs[1] + .coefs[2] * x + .coefs[3] * x^2 } boot_contrasts &lt;- function(data, indices) { .df &lt;- data[indices, ] coefs &lt;- lm_robust(wt82_71 ~ smkintensity82_71 + I(smkintensity82_71^2), data = .df, weights = swts) %&gt;% tidy() %&gt;% pull(estimate) c(calculate_contrast(coefs, 0), calculate_contrast(coefs, 20)) } bootstrap_contrasts &lt;- nhefs_light_smokers %&gt;% boot(boot_contrasts, R = 2000) bootstrap_contrasts %&gt;% tidy(conf.int = TRUE, conf.meth = &quot;bca&quot;) ## # A tibble: 2 x 5 ## statistic bias std.error conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.00 -0.0338 0.300 1.44 2.63 ## 2 0.903 0.221 1.38 -1.27 3.88 2.5 Program 12.5 Fitting a marginal structural model for a binary outcome is almost identical to fitting one for a continuous expsure, but we need to use a model appropriate for binary outcomes. We’ll use logistic regression and fit robust standard errors using geeglm(). The weightas, swts, are the same ones we used in Program 12.3. logistic_msm &lt;- geeglm( death ~ qsmk, data = nhefs_complete, family = binomial(), weights = swts, id = id ) tidy(logistic_msm, conf.int = TRUE, exponentiate = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.225 0.0789 356. 0 0.193 0.263 ## 2 qsmk 1.03 0.157 0.0367 0.848 0.757 1.40 2.6 Program 12.6 While the workflow for marginal structural models with interaction terms is similar to the above, there is one important difference: We need to include the interaction variable in both the numerator and denominator models so that we can safely use it in the marginal structural model. # use a model with sex as a predictor for the numerator numerator_sex &lt;- glm(qsmk ~ sex, data = nhefs_complete, family = binomial()) nhefs_complete &lt;- numerator_sex %&gt;% augment(type.predict = &quot;response&quot;, data = nhefs_complete %&gt;% select(-.fitted:-.std.resid)) %&gt;% mutate(numerator_sex = ifelse(qsmk == 0, 1 - .fitted, .fitted)) %&gt;% select(id, numerator_sex) %&gt;% left_join(nhefs_complete, by = &quot;id&quot;) %&gt;% mutate(swts_sex = numerator_sex * wts) Checking the weights is the same. nhefs_complete %&gt;% summarize(mean_wt = mean(swts_sex), sd_wts = sd(swts_sex)) ## # A tibble: 1 x 2 ## mean_wt sd_wts ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.999 0.271 ggplot(nhefs_complete, aes(swts_sex)) + geom_density(col = &quot;#E69F00&quot;, fill = &quot;#E69F0095&quot;) + scale_x_log10() + theme_minimal(base_size = 20) + xlab(&quot;log10(Stabilized Weights)&quot;) lm_robust(wt82_71 ~ qsmk*sex, data = nhefs_complete, weights = swts) %&gt;% tidy() ## term estimate std.error statistic p.value conf.low ## 1 (Intercept) 1.784446876 0.3101597 5.75331559 1.051124e-08 1.1760735 ## 2 qsmk 3.521977634 0.6585705 5.34791301 1.021696e-07 2.2302023 ## 3 sex1 -0.008724784 0.4492401 -0.01942121 9.845076e-01 -0.8899019 ## 4 qsmk:sex1 -0.159478525 1.0498718 -0.15190286 8.792832e-01 -2.2187851 ## conf.high df outcome ## 1 2.3928202 1562 wt82_71 ## 2 4.8137530 1562 wt82_71 ## 3 0.8724524 1562 wt82_71 ## 4 1.8998280 1562 wt82_71 2.7 Program 12.7 As you can see, the workflow for fitting marginal structural models follows a pattern: fight the weights, inverse or stabilize them, check the weights, and use them in a marginal model with the outcome and expsures of interest. Correcting for selection bias due to censoring uses the same work flow. Since we want to use both the censoring weights and the treatment weights, we can take their product and use the result to weight our marginal structural model. Since we’ll use stabilized weights, we have five models: the numerator and denominator for the censoring weights, the numerator and denominartor for the treatment weights, and the marginal structural model. # using complete data set nhefs_censored &lt;- nhefs %&gt;% drop_na(qsmk, sex, race, age, school, smokeintensity, smokeyrs, exercise, active, wt71) # Inverse Probability of Treatment Weights -------------------------------- numerator_sws_model &lt;- glm(qsmk ~ 1, data = nhefs_censored, family = binomial()) numerators_sws &lt;- numerator_sws_model %&gt;% augment(type.predict = &quot;response&quot;, data = nhefs_censored) %&gt;% mutate(numerator_sw = ifelse(qsmk == 0, 1 - .fitted, .fitted)) %&gt;% select(id, numerator_sw) denominator_sws_model &lt;- glm( qsmk ~ sex + race + age + I(age^2) + education + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), data = nhefs_censored, family = binomial() ) denominators_sws &lt;- denominator_sws_model %&gt;% augment(type.predict = &quot;response&quot;, data = nhefs_censored) %&gt;% mutate(denominator_sw = ifelse(qsmk == 0, 1 - .fitted, .fitted)) %&gt;% select(id, denominator_sw) # Inverse Probability of Censoring Weights -------------------------------- numerator_cens_model &lt;- glm(censored ~ qsmk, data = nhefs_censored, family = binomial()) numerators_cens &lt;- numerator_cens_model %&gt;% augment(type.predict = &quot;response&quot;, data = nhefs_censored) %&gt;% mutate(numerator_cens = ifelse(censored == 0, 1 - .fitted, 1)) %&gt;% select(id, numerator_cens) denominator_cens_model &lt;- glm( censored ~ qsmk + sex + race + age + I(age^2) + education + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), data = nhefs_censored, family = binomial() ) denominators_cens &lt;- denominator_cens_model %&gt;% augment(type.predict = &quot;response&quot;, data = nhefs_censored) %&gt;% mutate(denominator_cens = ifelse(censored == 0, 1 - .fitted, 1)) %&gt;% select(id, denominator_cens) # join all the weights data from above nhefs_censored_wts &lt;- nhefs_censored %&gt;% left_join(numerators_sws, by = &quot;id&quot;) %&gt;% left_join(denominators_sws, by = &quot;id&quot;) %&gt;% left_join(numerators_cens, by = &quot;id&quot;) %&gt;% left_join(denominators_cens, by = &quot;id&quot;) %&gt;% mutate( # IPTW swts = numerator_sw / denominator_sw, # IPCW cens_wts = numerator_cens / denominator_cens, # Multiply the weights to use in the model wts = swts * cens_wts ) The censoring weights are a little different but, since they are stabilized, they should still have a mean around 1. nhefs_censored_wts %&gt;% summarize(mean_wt = mean(cens_wts), sd_wts = sd(cens_wts)) ## # A tibble: 1 x 2 ## mean_wt sd_wts ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.999 0.0519 ggplot(nhefs_censored_wts, aes(cens_wts)) + geom_density(col = &quot;#E69F00&quot;, fill = &quot;#E69F0095&quot;) + scale_x_log10() + theme_minimal(base_size = 20) + xlab(&quot;log10(Stabilized Weights)&quot;) To fit both weights, we simply use their product in the marginal structural model. lm_robust(wt82_71 ~ qsmk, data = nhefs_censored_wts, weights = wts) %&gt;% tidy() ## term estimate std.error statistic p.value conf.low conf.high ## 1 (Intercept) 1.661990 0.2328693 7.137009 1.454358e-12 1.205221 2.118759 ## 2 qsmk 3.496493 0.5265564 6.640301 4.305944e-11 2.463662 4.529324 ## df outcome ## 1 1564 wt82_71 ## 2 1564 wt82_71 "],
["chapter-13-standardization-and-the-parametric-g-formula.html", "3 Chapter 13: Standardization and the Parametric G-Formula 3.1 Program 13.1 3.2 Program 13.2 3.3 Program 13.3 3.4 Program 13.4", " 3 Chapter 13: Standardization and the Parametric G-Formula This is the code for Chapter 13 library(tidyverse) library(haven) library(broom) library(boot) nhefs &lt;- read_sas(&quot;data/nhefs.sas7bdat&quot;) %&gt;% # add id and censored indicator # recode age &gt; 50 and years of school to categories mutate( id = 1:n(), censored = ifelse(is.na(wt82), 1, 0), older = case_when(is.na(age) ~ NA_real_, age &gt; 50 ~ 1, TRUE ~ 0), education = case_when(school &lt; 9 ~ 1, school &lt; 12 ~ 2, school == 12 ~ 3, school &lt; 16 ~ 4, TRUE ~ 5) ) %&gt;% # change categorical to factors mutate_at(vars(sex, race, education, exercise, active), factor) # restrict to complete cases nhefs_complete &lt;- nhefs %&gt;% drop_na(qsmk, sex, race, age, school, smokeintensity, smokeyrs, exercise, active, wt71, wt82, wt82_71, censored) 3.1 Program 13.1 standardized_model &lt;- glm( wt82_71 ~ qsmk + I(qsmk * smokeintensity) + smokeintensity + I(smokeintensity^2) + sex + race + age + I(age^2) + education + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), data = nhefs_complete ) # fit using the values of the covariates that this participant has standardized_model %&gt;% augment(newdata = filter(nhefs_complete, seqn == 24770)) %&gt;% select(.fitted) %&gt;% knitr::kable(digits = 2) .fitted 0.34 # predict on all combonations of covariates present in the data standardized_model %&gt;% augment() %&gt;% summarise_each(funs(mean, min, max), .fitted) %&gt;% knitr::kable(digits = 2) mean min max 2.64 -10.88 9.88 3.2 Program 13.2 zero &lt;- function(n) rep(0, n) one &lt;- function(n) rep(1, n) observed_data &lt;- tibble( name = c(&quot;Rheia&quot;, &quot;Kronos&quot;, &quot;Demeter&quot;, &quot;Hades&quot;, &quot;Hestia&quot;, &quot;Poseidon&quot;, &quot;Hera&quot;, &quot;Zeus&quot;, &quot;Artemis&quot;,&quot;Apollo&quot;, &quot;Leto&quot;, &quot;Ares&quot;, &quot;Athena&quot;, &quot;Hephaestus&quot;, &quot;Aphrodite&quot;, &quot;Cyclope&quot;, &quot;Persephone&quot;, &quot;Hermes&quot;, &quot;Hebe&quot;, &quot;Dionysus&quot;), l = c(zero(8), one(12)), a = c(zero(4), one(4), zero(3), one(9)), y = c(0, 1, zero(5), one(3), 0, one(6), zero(3)) ) knitr::kable(observed_data) name l a y Rheia 0 0 0 Kronos 0 0 1 Demeter 0 0 0 Hades 0 0 0 Hestia 0 1 0 Poseidon 0 1 0 Hera 0 1 0 Zeus 0 1 1 Artemis 1 0 1 Apollo 1 0 1 Leto 1 0 0 Ares 1 1 1 Athena 1 1 1 Hephaestus 1 1 1 Aphrodite 1 1 1 Cyclope 1 1 1 Persephone 1 1 1 Hermes 1 1 0 Hebe 1 1 0 Dionysus 1 1 0 untreated_data &lt;- observed_data %&gt;% mutate(a = 0) treated_data &lt;- observed_data %&gt;% mutate(a = 1) model_greeks &lt;- glm(y ~ a * l, data = observed_data) predicted_untreated &lt;- model_greeks %&gt;% augment(newdata = untreated_data) %&gt;% select(untreated = .fitted) predicted_treated &lt;- model_greeks %&gt;% augment(newdata = treated_data) %&gt;% select(treated = .fitted) bind_cols(predicted_untreated, predicted_treated) %&gt;% summarise( mean_treated = mean(treated), mean_untreated = mean(untreated), difference = mean_treated - mean_untreated ) ## # A tibble: 1 x 3 ## mean_treated mean_untreated difference ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.500 0.5 -1.67e-16 3.3 Program 13.3 kept_smoking &lt;- nhefs_complete %&gt;% mutate(qsmk = 0) quit_smoking &lt;- nhefs_complete %&gt;% mutate(qsmk = 1) predicted_kept_smoking &lt;- standardized_model %&gt;% augment(newdata = kept_smoking) %&gt;% select(kept_smoking = .fitted) predicted_quit_smoking &lt;- standardized_model %&gt;% augment(newdata = quit_smoking) %&gt;% select(quit_smoking = .fitted) bind_cols(predicted_kept_smoking, predicted_quit_smoking) %&gt;% summarise( mean_quit_smoking = mean(quit_smoking), mean_kept_smoking = mean(kept_smoking), difference = mean_quit_smoking - mean_kept_smoking ) ## # A tibble: 1 x 3 ## mean_quit_smoking mean_kept_smoking difference ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.27 1.76 3.52 3.4 Program 13.4 fit_gformula &lt;- function(data, indices) { .df &lt;- data[indices, ] standardized_model &lt;- glm( wt82_71 ~ qsmk + I(qsmk * smokeintensity) + smokeintensity + I(smokeintensity^2) + sex + race + age + I(age^2) + education + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), data = .df ) kept_smoking &lt;- nhefs_complete %&gt;% mutate(qsmk = 0) quit_smoking &lt;- nhefs_complete %&gt;% mutate(qsmk = 1) predicted_kept_smoking &lt;- standardized_model %&gt;% augment(newdata = kept_smoking) %&gt;% select(kept_smoking = .fitted) predicted_quit_smoking &lt;- standardized_model %&gt;% augment(newdata = quit_smoking) %&gt;% select(quit_smoking = .fitted) bind_cols(predicted_kept_smoking, predicted_quit_smoking) %&gt;% summarise( mean_quit_smoking = mean(quit_smoking), mean_kept_smoking = mean(kept_smoking), difference = mean_quit_smoking - mean_kept_smoking ) %&gt;% pull(difference) } bootstrapped_gformula &lt;- boot(nhefs_complete, fit_gformula, R = 2000) bootstrapped_gformula %&gt;% tidy(conf.int = TRUE, conf.meth = &quot;bca&quot;) ## # A tibble: 1 x 5 ## statistic bias std.error conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.52 -0.000109 0.490 2.61 4.49 "],
["chapter-14.html", "4 Chapter 14 4.1 Program 14.1 4.2 Program 14.2 4.3 Program 14.3", " 4 Chapter 14 This is the code for Chapter 14 4.1 Program 14.1 ranks &lt;- nhefs %&gt;% mutate( rank = min_rank(desc(wt82_71)), lbl = if_else( rank &lt;= 3 | rank &gt;= (max(rank, na.rm = TRUE) - 2), round(wt82_71, 1), NA_real_ ) ) %&gt;% select(seqn, rank, lbl, wt82_71) ranks %&gt;% select(-lbl) %&gt;% top_n(5, wt82_71) ## # A tibble: 5 x 3 ## seqn rank wt82_71 ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1769 3 37.7 ## 2 5415 5 34.0 ## 3 6928 2 47.5 ## 4 22342 4 37.0 ## 5 23522 1 48.5 ranks %&gt;% select(-lbl) %&gt;% top_n(-5, wt82_71) ## # A tibble: 5 x 3 ## seqn rank wt82_71 ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 5412 1563 -29.0 ## 2 13593 1565 -30.5 ## 3 21897 1562 -26.0 ## 4 23321 1566 -41.3 ## 5 24363 1564 -30.1 ranks %&gt;% ggplot(aes(y = rank, x = wt82_71)) + geom_vline(xintercept = 0, col = &quot;grey90&quot;, size = 1.3) + geom_point(col = &quot;#0072B2&quot;, size = 1, alpha = .9) + ggrepel::geom_text_repel( aes(label = lbl), size = 4, point.padding = 0.1, box.padding = .6, force = 1., min.segment.length = 0, seed = 777 ) + theme_minimal(14) + expand_limits(y = c(-200, 1700)) + xlab(&quot;change in weight&quot;) 4.2 Program 14.2 # using complete data set nhefs_censored &lt;- nhefs %&gt;% drop_na(qsmk, sex, race, age, school, smokeintensity, smokeyrs, exercise, active, wt71) # compute unstabilized inverse probability of censoring weights cwts_model &lt;- glm( censored ~ qsmk + sex + race + age + I(age^2) + education + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), data = nhefs_censored, family = binomial() ) nhefs_censored &lt;- cwts_model %&gt;% augment(type.predict = &quot;response&quot;, data = nhefs_censored) %&gt;% mutate(cwts = 1 / ifelse(censored == 0, 1 - .fitted, .fitted)) # compute all values of h(psi) compute_h_psi &lt;- function(psi) { df &lt;- nhefs_censored %&gt;% mutate(h_psi = wt82_71 - psi * qsmk) %&gt;% # gee doesn&#39;t like missing values drop_na(h_psi) geeglm( qsmk ~ h_psi + sex + race + age + I(age^2) + education + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), data = df, family = binomial(), std.err = &quot;san.se&quot;, weights = cwts, id = id, corstr = &quot;independence&quot; ) %&gt;% tidy() %&gt;% filter(term == &quot;h_psi&quot;) %&gt;% mutate(psi = psi) %&gt;% select(psi, estimate, p.value) } compute_h_psi(3.446) ## # A tibble: 1 x 3 ## psi estimate p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.45 -0.00000190 1.000 psi_search &lt;- map_dfr(seq(2, 5, by = .1), compute_h_psi) psi_search %&gt;% arrange(abs(estimate)) ## # A tibble: 31 x 3 ## psi estimate p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.4 0.000862 0.922 ## 2 3.5 -0.00102 0.909 ## 3 3.3 0.00274 0.755 ## 4 3.6 -0.00290 0.744 ## 5 3.2 0.00461 0.598 ## 6 3.7 -0.00480 0.592 ## 7 3.1 0.00647 0.457 ## 8 3.8 -0.00670 0.457 ## 9 3 0.00833 0.337 ## 10 3.9 -0.00860 0.342 ## # … with 21 more rows psi_est &lt;- psi_search %&gt;% arrange(abs(estimate)) %&gt;% slice(1) %&gt;% select(-estimate, -p.value) # get minimum and maximum values that have p &gt;= .05 for confidence intervals psi_conf_int &lt;- psi_search %&gt;% filter(p.value &gt;= .05) %&gt;% slice(c(1, n())) %&gt;% mutate(type = c(&quot;conf.low&quot;, &quot;conf.high&quot;)) %&gt;% select(type, psi) %&gt;% spread(type, psi) bind_cols(psi_est, psi_conf_int) ## # A tibble: 1 x 3 ## psi conf.high conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.4 4.4 2.6 psi_search %&gt;% ggplot(aes(x = psi, y = estimate)) + geom_hline(yintercept = 0, col = &quot;grey85&quot;, size = 1.3) + geom_line(col = &quot;#0072B2&quot;, size = 1.2) + geom_point(shape = 21, col = &quot;white&quot;, fill = &quot;#0072B2&quot;, size = 2.5) + theme_minimal(14) psi_formula &lt;- function(weights, outcome, treatment, treatment_pred) { numerator &lt;- weights * outcome * (treatment - treatment_pred) denominator &lt;- sum(weights * treatment * (treatment - treatment_pred), na.rm = TRUE) sum(numerator / denominator, na.rm = TRUE) } estimate_psi &lt;- function(.data) { glm( qsmk ~ sex + race + age + I(age^2) + education + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), data = .data, family = binomial(), weights = cwts, ) %&gt;% augment(data = .data, type.predict = &quot;response&quot;) %&gt;% summarize( psi = psi_formula( weights = cwts, outcome = wt82_71, treatment = qsmk, treatment_pred = .fitted ) ) } nhefs_censored %&gt;% select(-.fitted:-.cooksd) %&gt;% filter(censored == 0) %&gt;% estimate_psi() ## # A tibble: 1 x 1 ## psi ## &lt;dbl&gt; ## 1 3.45 bootstrap_psi &lt;- function(data, indices) { estimate_psi(data[indices, ]) %&gt;% pull(psi) } bootstrapped_psis &lt;- nhefs_censored %&gt;% select(-.fitted:-.cooksd) %&gt;% filter(censored == 0) %&gt;% boot(bootstrap_psi, R = 2000) bootstrapped_psis %&gt;% tidy(conf.int = TRUE, conf.method = &quot;bca&quot;) ## # A tibble: 1 x 5 ## statistic bias std.error conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.45 -0.00202 0.473 2.51 4.38 4.3 Program 14.3 estimate_psi2 &lt;- function(.data) { glm( qsmk ~ sex + race + age + I(age^2) + education + smokeintensity + I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), data = .data, family = binomial(), weights = cwts, ) %&gt;% augment(data = .data, type.predict = &quot;response&quot;) %&gt;% psi_formula2() } solve_matrix &lt;- function(.data, .names = c(&quot;psi1&quot;, &quot;psi2&quot;)) { cells &lt;- .data %&gt;% summarise( a1 = sum(qsmk * diff), a2 = sum(qsmk * smokeintensity * diff), a3 = sum(qsmk * smokeintensity * diff), a4 = sum(qsmk * smokeintensity * smokeintensity * diff), b1 = sum(wt82_71 * diff), b2 = sum(wt82_71 * smokeintensity * diff) ) a &lt;- cells %&gt;% select(a1:a4) %&gt;% unlist() %&gt;% matrix(2, 2) b &lt;- cells %&gt;% select(b1:b2) %&gt;% unlist() %&gt;% matrix(2, 1) solve(a, b) %&gt;% t() %&gt;% as_tibble(.name_repair = &quot;minimal&quot;) %&gt;% set_names(.names) } psi_formula2 &lt;- function(.data) { .data %&gt;% mutate(diff = (qsmk - .fitted) * cwts) %&gt;% drop_na(wt82_71) %&gt;% solve_matrix() } nhefs_censored %&gt;% select(-.fitted:-.cooksd) %&gt;% filter(censored == 0) %&gt;% estimate_psi2() ## # A tibble: 1 x 2 ## psi1 psi2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2.86 0.0300 "],
["chapter-15.html", "5 Chapter 15", " 5 Chapter 15 This is the code for Chapter 15 "],
["chapter-16.html", "6 Chapter 16", " 6 Chapter 16 This is the code for Chapter 16 "],
["chapter-17.html", "7 Chapter 17", " 7 Chapter 17 This is the code for Chapter 11 "]
]
